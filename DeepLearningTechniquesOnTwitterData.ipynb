{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Word2Vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "load the python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "from keras.models import Sequential\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from keras.layers import Activation, Dense\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.preprocessing import scale\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.sampledata.periodic_table import elements"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Defining a function that loads the dataset and extracts the two columns we need:\n",
    "* The sentiment: a binary (0/1) variable\n",
    "* The text of the tweet: string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training and Testing setÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, string\n",
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "import re, string\n",
    "def clean_str(string):\n",
    "  \"\"\"\n",
    "  String cleaning before vectorization\n",
    "  \"\"\"\n",
    "  try:\n",
    "    string = re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    #string = re.sub(r\"[^A-Za-z]\", \" \", string) \n",
    "    string = re.sub(r\"\\W+\", \" \", string)\n",
    "    words = string.strip().lower().split()    \n",
    "    words = [w for w in words if len(w)>1]\n",
    "    #return words list\n",
    "    return words\n",
    "  except:\n",
    "    print (traceback.print_exc())\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'tweets_sentiment.txt' does not exist: b'tweets_sentiment.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a9918a612401>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load training and Test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets_sentiment.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'tweets_sentiment.txt' does not exist: b'tweets_sentiment.txt'"
     ]
    }
   ],
   "source": [
    "#Load training and Test data\n",
    "import pandas as pd\n",
    "data=pd.read_csv('tweets_sentiment.txt', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-304fa4ce4ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=df[df.str.contains('macron' or 'emmanuelmacron' or 'enmarche' or '@emmanuelmacron' or '#emmanuelmacron' or 'mlpofficiel' or 'mlp_officiel' or 'lepen' or 'marine2017' or '@mlp_officiel' or 'mlp',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574555"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text2']=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ah macron et ses approximations</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ah macron et ses approximations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a marseille fillon cible macron lepen et mÃ©le...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a marseille fillon cible macron lepen et mÃ©le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>face mÃ©lenchon et son programme de destruction...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>face mÃ©lenchon et son programme de destruction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sÃ©rieusement venant des soutiens de macron l Ã©...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>sÃ©rieusement venant des soutiens de macron l Ã©...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "2                    ah macron et ses approximations         1.0   \n",
       "9    a marseille fillon cible macron lepen et mÃ©le...        1.0   \n",
       "16  face mÃ©lenchon et son programme de destruction...        1.0   \n",
       "18   macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...        1.0   \n",
       "39  sÃ©rieusement venant des soutiens de macron l Ã©...        1.0   \n",
       "\n",
       "                                                text2  \n",
       "2                    ah macron et ses approximations   \n",
       "9    a marseille fillon cible macron lepen et mÃ©le...  \n",
       "16  face mÃ©lenchon et son programme de destruction...  \n",
       "18   macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...  \n",
       "39  sÃ©rieusement venant des soutiens de macron l Ã©...  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']=data['text2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['text2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ah macron et ses approximations</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a marseille fillon cible macron lepen et mÃ©le...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>face mÃ©lenchon et son programme de destruction...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sÃ©rieusement venant des soutiens de macron l Ã©...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "2                    ah macron et ses approximations         1.0\n",
       "9    a marseille fillon cible macron lepen et mÃ©le...        1.0\n",
       "16  face mÃ©lenchon et son programme de destruction...        1.0\n",
       "18   macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...        1.0\n",
       "39  sÃ©rieusement venant des soutiens de macron l Ã©...        1.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleanText'] = data['text'].apply(clean_str)\n",
    "data.sentiment=data.sentiment.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Sentiment']=data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SentimentText']=data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens']=data['cleanText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ah macron et ses approximations</td>\n",
       "      <td>1</td>\n",
       "      <td>[ah, macron, et, ses, approximations]</td>\n",
       "      <td>1</td>\n",
       "      <td>ah macron et ses approximations</td>\n",
       "      <td>[ah, macron, et, ses, approximations]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a marseille fillon cible macron lepen et mÃ©le...</td>\n",
       "      <td>1</td>\n",
       "      <td>[marseille, fillon, cible, macron, lepen, et, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a marseille fillon cible macron lepen et mÃ©le...</td>\n",
       "      <td>[marseille, fillon, cible, macron, lepen, et, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>face mÃ©lenchon et son programme de destruction...</td>\n",
       "      <td>1</td>\n",
       "      <td>[face, lenchon, et, son, programme, de, destru...</td>\n",
       "      <td>1</td>\n",
       "      <td>face mÃ©lenchon et son programme de destruction...</td>\n",
       "      <td>[face, lenchon, et, son, programme, de, destru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...</td>\n",
       "      <td>1</td>\n",
       "      <td>[macron, fait, fl, chir, sur, quand, lenchon, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...</td>\n",
       "      <td>[macron, fait, fl, chir, sur, quand, lenchon, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sÃ©rieusement venant des soutiens de macron l Ã©...</td>\n",
       "      <td>1</td>\n",
       "      <td>[rieusement, venant, des, soutiens, de, macron...</td>\n",
       "      <td>1</td>\n",
       "      <td>sÃ©rieusement venant des soutiens de macron l Ã©...</td>\n",
       "      <td>[rieusement, venant, des, soutiens, de, macron...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment  \\\n",
       "2                    ah macron et ses approximations           1   \n",
       "9    a marseille fillon cible macron lepen et mÃ©le...          1   \n",
       "16  face mÃ©lenchon et son programme de destruction...          1   \n",
       "18   macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...          1   \n",
       "39  sÃ©rieusement venant des soutiens de macron l Ã©...          1   \n",
       "\n",
       "                                            cleanText  Sentiment  \\\n",
       "2               [ah, macron, et, ses, approximations]          1   \n",
       "9   [marseille, fillon, cible, macron, lepen, et, ...          1   \n",
       "16  [face, lenchon, et, son, programme, de, destru...          1   \n",
       "18  [macron, fait, fl, chir, sur, quand, lenchon, ...          1   \n",
       "39  [rieusement, venant, des, soutiens, de, macron...          1   \n",
       "\n",
       "                                        SentimentText  \\\n",
       "2                    ah macron et ses approximations    \n",
       "9    a marseille fillon cible macron lepen et mÃ©le...   \n",
       "16  face mÃ©lenchon et son programme de destruction...   \n",
       "18   macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...   \n",
       "39  sÃ©rieusement venant des soutiens de macron l Ã©...   \n",
       "\n",
       "                                               tokens  \n",
       "2               [ah, macron, et, ses, approximations]  \n",
       "9   [marseille, fillon, cible, macron, lepen, et, ...  \n",
       "16  [face, lenchon, et, son, programme, de, destru...  \n",
       "18  [macron, fait, fl, chir, sur, quand, lenchon, ...  \n",
       "39  [rieusement, venant, des, soutiens, de, macron...  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to delete unwanted columns\n",
    "del data['cleanText']\n",
    "del data['sentiment']\n",
    "del data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ah macron et ses approximations</td>\n",
       "      <td>[ah, macron, et, ses, approximations]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>a marseille fillon cible macron lepen et mÃ©le...</td>\n",
       "      <td>[marseille, fillon, cible, macron, lepen, et, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>face mÃ©lenchon et son programme de destruction...</td>\n",
       "      <td>[face, lenchon, et, son, programme, de, destru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...</td>\n",
       "      <td>[macron, fait, fl, chir, sur, quand, lenchon, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>sÃ©rieusement venant des soutiens de macron l Ã©...</td>\n",
       "      <td>[rieusement, venant, des, soutiens, de, macron...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                                      SentimentText  \\\n",
       "2           1                   ah macron et ses approximations    \n",
       "9           1   a marseille fillon cible macron lepen et mÃ©le...   \n",
       "16          1  face mÃ©lenchon et son programme de destruction...   \n",
       "18          1   macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...   \n",
       "39          1  sÃ©rieusement venant des soutiens de macron l Ã©...   \n",
       "\n",
       "                                               tokens  \n",
       "2               [ah, macron, et, ses, approximations]  \n",
       "9   [marseille, fillon, cible, macron, lepen, et, ...  \n",
       "16  [face, lenchon, et, son, programme, de, destru...  \n",
       "18  [macron, fait, fl, chir, sur, quand, lenchon, ...  \n",
       "39  [rieusement, venant, des, soutiens, de, macron...  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1086909</th>\n",
       "      <td>1</td>\n",
       "      <td>pol is getting closer to blowing the macronga...</td>\n",
       "      <td>[pol, is, getting, closer, to, blowing, the, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086912</th>\n",
       "      <td>-1</td>\n",
       "      <td>macron ne veut pas se donner les moyens de lu...</td>\n",
       "      <td>[macron, ne, veut, pas, se, donner, les, moyen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086913</th>\n",
       "      <td>1</td>\n",
       "      <td>because he hasn t backed enough losers over t...</td>\n",
       "      <td>[because, he, hasn, backed, enough, losers, ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086914</th>\n",
       "      <td>1</td>\n",
       "      <td>the kiss of death for macron obama</td>\n",
       "      <td>[the, kiss, of, death, for, macron, obama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086915</th>\n",
       "      <td>1</td>\n",
       "      <td>concernant les propos de macron sur la refonda...</td>\n",
       "      <td>[concernant, les, propos, de, macron, sur, la,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment                                      SentimentText  \\\n",
       "1086909          1   pol is getting closer to blowing the macronga...   \n",
       "1086912         -1   macron ne veut pas se donner les moyens de lu...   \n",
       "1086913          1   because he hasn t backed enough losers over t...   \n",
       "1086914          1                 the kiss of death for macron obama   \n",
       "1086915          1  concernant les propos de macron sur la refonda...   \n",
       "\n",
       "                                                    tokens  \n",
       "1086909  [pol, is, getting, closer, to, blowing, the, m...  \n",
       "1086912  [macron, ne, veut, pas, se, donner, les, moyen...  \n",
       "1086913  [because, he, hasn, backed, enough, losers, ov...  \n",
       "1086914         [the, kiss, of, death, for, macron, obama]  \n",
       "1086915  [concernant, les, propos, de, macron, sur, la,...  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The data is now tokenized and cleaned. We are ready to feed it in the word2vec model.\n",
    "First, let's define a training set and a test set and manual validation test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574555"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>574550</th>\n",
       "      <td>1</td>\n",
       "      <td>pol is getting closer to blowing the macronga...</td>\n",
       "      <td>[pol, is, getting, closer, to, blowing, the, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574551</th>\n",
       "      <td>-1</td>\n",
       "      <td>macron ne veut pas se donner les moyens de lu...</td>\n",
       "      <td>[macron, ne, veut, pas, se, donner, les, moyen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574552</th>\n",
       "      <td>1</td>\n",
       "      <td>because he hasn t backed enough losers over t...</td>\n",
       "      <td>[because, he, hasn, backed, enough, losers, ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574553</th>\n",
       "      <td>1</td>\n",
       "      <td>the kiss of death for macron obama</td>\n",
       "      <td>[the, kiss, of, death, for, macron, obama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574554</th>\n",
       "      <td>1</td>\n",
       "      <td>concernant les propos de macron sur la refonda...</td>\n",
       "      <td>[concernant, les, propos, de, macron, sur, la,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                      SentimentText  \\\n",
       "574550          1   pol is getting closer to blowing the macronga...   \n",
       "574551         -1   macron ne veut pas se donner les moyens de lu...   \n",
       "574552          1   because he hasn t backed enough losers over t...   \n",
       "574553          1                 the kiss of death for macron obama   \n",
       "574554          1  concernant les propos de macron sur la refonda...   \n",
       "\n",
       "                                                   tokens  \n",
       "574550  [pol, is, getting, closer, to, blowing, the, m...  \n",
       "574551  [macron, ne, veut, pas, se, donner, les, moyen...  \n",
       "574552  [because, he, hasn, backed, enough, losers, ov...  \n",
       "574553         [the, kiss, of, death, for, macron, obama]  \n",
       "574554  [concernant, les, propos, de, macron, sur, la,...  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=data.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2=data.tail(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Data For Manual Validation:\n",
    "writer = pd.ExcelWriter('vld.xlsx')\n",
    "d1.to_excel(writer,'Sheet1')\n",
    "d2.to_excel(writer,'Sheet2')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[500:574054]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>574049</th>\n",
       "      <td>1</td>\n",
       "      <td>l escroquerie du vote macron via</td>\n",
       "      <td>[escroquerie, du, vote, macron, via]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574050</th>\n",
       "      <td>-1</td>\n",
       "      <td>jevotepour macron car je fais partie des crÃ©t...</td>\n",
       "      <td>[jevotepour, macron, car, je, fais, partie, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574051</th>\n",
       "      <td>1</td>\n",
       "      <td>les soutiens de m macron agissent dans la vio...</td>\n",
       "      <td>[les, soutiens, de, macron, agissent, dans, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574052</th>\n",
       "      <td>1</td>\n",
       "      <td>macron liberalisme pas gauche xtreme droite a...</td>\n",
       "      <td>[macron, liberalisme, pas, gauche, xtreme, dro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574053</th>\n",
       "      <td>1</td>\n",
       "      <td>as obama implored nigerians to vote right as ...</td>\n",
       "      <td>[as, obama, implored, nigerians, to, vote, rig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment                                      SentimentText  \\\n",
       "574049          1                  l escroquerie du vote macron via    \n",
       "574050         -1   jevotepour macron car je fais partie des crÃ©t...   \n",
       "574051          1   les soutiens de m macron agissent dans la vio...   \n",
       "574052          1   macron liberalisme pas gauche xtreme droite a...   \n",
       "574053          1   as obama implored nigerians to vote right as ...   \n",
       "\n",
       "                                                   tokens  \n",
       "574049               [escroquerie, du, vote, macron, via]  \n",
       "574050  [jevotepour, macron, car, je, fais, partie, de...  \n",
       "574051  [les, soutiens, de, macron, agissent, dans, la...  \n",
       "574052  [macron, liberalisme, pas, gauche, xtreme, dro...  \n",
       "574053  [as, obama, implored, nigerians, to, vote, rig...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1000000\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),np.array(data.head(n).Sentiment), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ list(['malgr', 'son', 'retrait', 'de', 'la', 'vie', 'politique', 'barack', 'obama', 'affich', 'officiellement', 'sa', 'confiance', 'emmanuel', 'macron', 'dans']),\n",
       "       list(['neelie', 'kroes', 'la', 'dirigeante', 'une', 'entreprise', 'offshore', 'aux', 'bahamas', 'rencontr', 'macron', 'lasvegas', 'en', 'ja']),\n",
       "       list(['macron']), ...,\n",
       "       list(['attali', 'le', 'mentor', 'de', 'macron', 'vous', 'explique', 'son', 'projet', 'pour', 'la', 'france']),\n",
       "       list(['macron', 'rothschild', 'banker', 'bilderberg', 'member', 'bureaucrat', 'in', 'hollande', 'government', 'media', 'he', 'an', 'outsider', 'this', 'is', 'lite']),\n",
       "       list(['mme', 'lepen', 'que', 'proposez', 'vous', 'pour', 'duire', 'le', 'ch', 'mage', 'regardez', 'est', 'le', 'score', 'sur', 'candy', 'crush', 'de', 'monsieur', 'macron'])], dtype=object)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Before feeding lists of tokens into the word2vec model, we must turn them into LabeledSentence objects beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "458843it [00:02, 156452.57it/s]\n",
      "114711it [00:01, 70520.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized\n",
    "\n",
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "first element from x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['malgr', 'son', 'retrait', 'de', 'la', 'vie', 'politique', 'barack', 'obama', 'affich', 'officiellement', 'sa', 'confiance', 'emmanuel', 'macron', 'dans'], tags=['TRAIN_0'])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the word2vec model from x_train i.e. the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 11:29:59,988 : INFO : loading projection weights from frWiki.bin\n",
      "2017-08-24 11:30:05,740 : INFO : loaded (66274, 1000) matrix from frWiki.bin\n"
     ]
    }
   ],
   "source": [
    "frenchW2V = gensim.models.KeyedVectors.load_word2vec_format('frWiki.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66274, 1000)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchW2V.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "* On the first line the model is initialized with the dimension of the vector space (we set it to 200) and min_count (a threshold for filtering words that appear less)\n",
    "* On the second line the vocabulary is created.\n",
    "* On the third line the model is trained i.e. its weights are updated."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Once the model is built and trained on the corpus of tweets, we can use it to convert words to vectors. \n",
    "Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.92278284e-02,  -1.21879382e-02,  -5.50427884e-02,\n",
       "         5.55902123e-02,   6.10539652e-02,   9.64832902e-02,\n",
       "        -1.28307529e-02,  -1.15820607e-02,  -2.21761074e-02,\n",
       "        -9.99968778e-03,   2.94949394e-04,  -1.47818178e-02,\n",
       "        -7.75420740e-02,   2.75168717e-02,   3.78509611e-02,\n",
       "        -1.96574461e-02,   5.44788241e-02,   5.37251383e-02,\n",
       "        -2.15995288e-03,   2.10342873e-02,  -1.45163193e-01,\n",
       "         9.81824193e-03,   8.27191770e-02,   2.56233085e-02,\n",
       "        -1.69443563e-02,   6.22188821e-02,  -4.27901261e-02,\n",
       "         8.07462446e-03,  -1.06191412e-01,  -2.75264215e-03,\n",
       "        -1.54654197e-02,   4.38642316e-03,  -1.45791203e-01,\n",
       "        -9.34110880e-02,  -4.71170284e-02,  -1.21728517e-02,\n",
       "        -4.58486751e-02,   6.07920513e-02,   1.63038164e-01,\n",
       "         3.35928723e-02,   1.49394006e-01,   1.34771377e-01,\n",
       "        -1.79264750e-02,   3.60278673e-02,  -3.77434725e-03,\n",
       "         2.10834350e-02,  -4.23749583e-03,  -3.64756063e-02,\n",
       "        -4.60192896e-02,  -5.76168895e-02,  -7.01192245e-02,\n",
       "         5.21208867e-02,   6.78243041e-02,   7.22004101e-02,\n",
       "        -3.28047946e-02,  -6.06874898e-02,  -1.22067975e-02,\n",
       "        -1.93073470e-02,   3.21823768e-02,   1.37511820e-01,\n",
       "        -8.27789865e-03,  -7.22372532e-02,  -1.60604626e-01,\n",
       "        -6.55319691e-02,  -1.50955960e-01,   7.34641030e-02,\n",
       "         1.84534248e-02,   2.55324189e-02,   9.48419794e-02,\n",
       "         7.82123059e-02,  -1.39938220e-02,  -7.57576823e-02,\n",
       "         4.24897224e-02,  -4.59360033e-02,   1.49139641e-02,\n",
       "         2.57849246e-02,  -5.89758046e-02,  -9.66574326e-02,\n",
       "         1.31776661e-01,   5.93681401e-03,  -6.53983057e-02,\n",
       "        -7.10526556e-02,  -3.72632705e-02,   7.07472907e-03,\n",
       "         1.62621126e-01,  -1.14018554e-02,   7.35496804e-02,\n",
       "         2.56747603e-02,   1.18240923e-01,   1.90525837e-02,\n",
       "         1.21983863e-01,   3.68161052e-02,   1.05869219e-01,\n",
       "         6.45870566e-02,  -1.82846151e-02,   3.63117903e-02,\n",
       "        -8.97489190e-02,  -5.92630133e-02,   6.87214956e-02,\n",
       "        -9.42867398e-02,  -4.76896279e-02,   3.14430818e-02,\n",
       "        -5.22435233e-02,  -9.85907614e-02,   1.37446802e-02,\n",
       "         9.66615602e-02,   2.18511701e-01,  -5.27995713e-02,\n",
       "         3.22069116e-02,  -3.25473808e-02,  -5.62668964e-02,\n",
       "         6.49344400e-02,  -6.60499856e-02,   1.15520507e-02,\n",
       "         8.79575014e-02,  -4.72655408e-02,  -3.53231952e-02,\n",
       "         3.61930430e-02,   1.20160431e-02,   4.76020388e-02,\n",
       "        -4.68063168e-02,   3.15476879e-02,   1.21243052e-01,\n",
       "        -1.04036637e-01,  -7.76884332e-02,   5.52882366e-02,\n",
       "        -4.82102856e-03,   1.63221546e-02,  -2.54790559e-02,\n",
       "         9.14469883e-02,   1.03040016e-03,  -9.11871642e-02,\n",
       "         6.89760447e-02,   1.13181315e-01,   2.91878767e-02,\n",
       "         3.89869628e-03,  -4.08063494e-02,  -4.01813313e-02,\n",
       "         3.70011553e-02,  -4.91641760e-02,  -3.01259030e-02,\n",
       "        -2.81722490e-02,  -9.96442288e-02,   2.97166547e-03,\n",
       "         1.93110332e-02,   1.78735517e-02,  -1.17939688e-01,\n",
       "        -1.02207743e-01,  -4.43975739e-02,  -5.38259596e-02,\n",
       "        -2.27126852e-02,  -1.38049936e-02,  -3.68979461e-02,\n",
       "        -1.12308534e-02,   4.03565280e-02,  -7.54779801e-02,\n",
       "        -8.40805694e-02,   1.05692916e-01,  -3.54767442e-02,\n",
       "        -4.75577004e-02,   1.49553921e-03,   6.84962347e-02,\n",
       "         1.85688399e-02,   4.75362055e-02,   5.36197051e-03,\n",
       "        -7.71963745e-02,   6.79854155e-02,  -1.43395126e-01,\n",
       "         4.52873483e-02,  -3.13938148e-02,   1.39682470e-02,\n",
       "        -6.02465235e-02,  -7.34171197e-02,  -4.39507477e-02,\n",
       "        -4.58600000e-02,   9.78204757e-02,   4.84564528e-02,\n",
       "         4.78256717e-02,   1.49167562e-02,   3.55610773e-02,\n",
       "        -3.46979313e-02,  -2.37834267e-02,  -2.31771227e-02,\n",
       "        -1.36171401e-01,  -4.59429994e-02,  -1.80344693e-02,\n",
       "        -4.85934392e-02,  -5.20868264e-02,   6.06265962e-02,\n",
       "        -9.62787122e-02,  -7.92014822e-02,  -8.52134302e-02,\n",
       "        -4.53556888e-03,   5.45462631e-02,   2.24914607e-02,\n",
       "         9.19513777e-02,   1.72429085e-02,   1.96965914e-02,\n",
       "        -3.66387442e-02,  -2.26891581e-02,   3.23706828e-02,\n",
       "         1.37283444e-01,   5.26048616e-02,   2.39045452e-02,\n",
       "         5.85155822e-02,   2.27122419e-02,  -9.30899940e-03,\n",
       "        -4.15919572e-02,  -4.55048010e-02,  -1.32525921e-01,\n",
       "        -1.07138008e-02,  -3.89869176e-02,   1.01103541e-02,\n",
       "        -1.74836516e-02,   4.99146059e-02,   2.11929455e-02,\n",
       "         8.28798767e-03,  -3.04640532e-02,   1.66848805e-02,\n",
       "         9.97605845e-02,   1.09704696e-02,   1.11513743e-02,\n",
       "        -4.27357992e-03,  -6.71085939e-02,   3.17034987e-03,\n",
       "        -1.18099395e-02,  -1.82736535e-02,  -5.45159206e-02,\n",
       "        -3.18062678e-02,  -7.18997419e-02,   8.75279158e-02,\n",
       "         7.32531846e-02,  -7.01265261e-02,  -2.39021257e-02,\n",
       "         1.05219118e-01,   2.07719598e-02,  -4.83080260e-02,\n",
       "        -1.58745260e-03,   1.95276402e-02,   5.26552200e-02,\n",
       "        -5.18756025e-02,  -1.03811219e-01,  -1.21241175e-02,\n",
       "        -3.52475271e-02,  -9.00550373e-03,  -4.03381623e-02,\n",
       "         4.40594852e-02,   5.39913513e-02,   8.60852003e-02,\n",
       "         3.96796539e-02,   2.43511405e-02,   3.84999737e-02,\n",
       "         6.09433874e-02,   8.66630897e-02,   4.75610867e-02,\n",
       "         3.21740359e-02,   8.09636414e-02,   7.34686926e-02,\n",
       "        -6.47209957e-03,  -4.22690138e-02,  -2.46785618e-02,\n",
       "         7.11507201e-02,   3.17811733e-03,   2.54076347e-02,\n",
       "         3.98191363e-02,   2.40494963e-02,  -2.16478016e-02,\n",
       "        -8.62513576e-03,  -5.72306029e-02,  -2.43059848e-03,\n",
       "        -1.00325890e-01,   2.65087411e-02,   2.90225502e-02,\n",
       "         2.32944023e-02,  -4.34934385e-02,  -1.29058864e-02,\n",
       "        -2.52370108e-02,  -1.13080861e-02,   2.10992042e-02,\n",
       "         2.77687665e-02,  -3.71489264e-02,  -9.26610455e-02,\n",
       "         5.60180992e-02,  -4.11562854e-03,   8.88162665e-03,\n",
       "        -5.70102409e-02,  -9.02568549e-02,  -4.40246193e-03,\n",
       "        -1.15494899e-01,  -9.77286026e-02,   1.51504368e-01,\n",
       "         5.00757853e-03,  -4.37813699e-02,   7.23337242e-03,\n",
       "        -3.75039713e-03,  -4.13145460e-02,   4.02996093e-02,\n",
       "         4.35652100e-02,   4.08869535e-02,   4.09772545e-02,\n",
       "         1.29782200e-01,  -9.03824791e-02,  -6.89216107e-02,\n",
       "        -1.13002598e-01,   2.67846510e-02,   4.91591543e-03,\n",
       "        -6.57603983e-03,  -3.32410767e-04,   5.82518205e-02,\n",
       "        -6.33950830e-02,   3.51845585e-02,  -6.37796754e-03,\n",
       "         7.39068836e-02,   3.58140394e-02,  -5.24394587e-02,\n",
       "        -1.43602034e-02,   3.88179868e-02,  -7.32924491e-02,\n",
       "         3.89966518e-02,   1.38006695e-02,  -8.60432312e-02,\n",
       "        -3.19591537e-02,  -3.02759465e-02,  -3.36452536e-02,\n",
       "        -1.69905405e-02,   1.53491810e-01,  -2.72360425e-02,\n",
       "        -3.04034855e-02,  -5.62655032e-02,   9.14093703e-02,\n",
       "         6.36586696e-02,  -8.55968893e-02,  -1.37009555e-02,\n",
       "        -3.68029885e-02,   6.60487413e-02,  -6.50365055e-02,\n",
       "         8.83524790e-02,   1.00454763e-01,  -5.82752973e-02,\n",
       "        -2.91989613e-02,  -3.23060304e-02,   5.92289120e-02,\n",
       "        -1.94671971e-03,   3.34085748e-02,   3.48880216e-02,\n",
       "         3.88915986e-02,   2.46743523e-02,   2.12595221e-02,\n",
       "         2.17764005e-02,   6.61620796e-02,  -4.56394143e-02,\n",
       "         4.08956856e-02,  -7.47020394e-02,   1.47116790e-03,\n",
       "        -3.82044003e-03,  -5.15350550e-02,   1.95950828e-03,\n",
       "        -5.22590093e-02,   1.08239271e-01,  -5.67937493e-02,\n",
       "         6.01920635e-02,  -8.34945515e-02,  -7.88083076e-02,\n",
       "         2.54023317e-02,   4.98742536e-02,  -6.18844246e-03,\n",
       "         5.83011545e-02,   2.95358454e-03,  -1.17041476e-01,\n",
       "         4.70459647e-03,   1.70685500e-01,  -6.56123385e-02,\n",
       "         3.75999650e-03,   1.07128963e-01,   9.76599678e-02,\n",
       "        -1.74354110e-02,  -1.42381378e-02,   2.83699967e-02,\n",
       "         1.00093685e-01,   7.28092194e-02,   3.68355657e-03,\n",
       "         5.52370176e-02,   6.17053993e-02,  -5.67469895e-02,\n",
       "         2.45664977e-02,   4.47092205e-02,   4.53702770e-02,\n",
       "        -4.05497989e-03,  -7.49259349e-03,   1.84519701e-02,\n",
       "        -2.23185346e-02,   7.31545985e-02,   4.49304469e-02,\n",
       "         6.25200421e-02,  -9.15722698e-02,   6.20569848e-02,\n",
       "        -1.67054422e-02,   1.07530996e-01,   8.85084271e-02,\n",
       "        -5.68394288e-02,  -1.54375406e-02,   3.89316492e-02,\n",
       "        -1.34081328e-02,   4.26429510e-02,  -3.71403433e-02,\n",
       "         6.52274117e-02,   6.70932457e-02,  -1.04239471e-01,\n",
       "         1.95730161e-02,  -8.59865993e-02,  -3.08056381e-02,\n",
       "         5.18757328e-02,  -1.24064572e-02,  -1.32072479e-01,\n",
       "         1.12743974e-02,   4.02677432e-02,  -1.51786670e-01,\n",
       "         1.05861820e-01,  -9.70311910e-02,   1.32468957e-02,\n",
       "         3.90164740e-03,  -2.28638556e-02,  -8.93788263e-02,\n",
       "         2.14859601e-02,   8.43923092e-02,  -5.35159446e-02,\n",
       "        -1.79220170e-01,   1.98485181e-02,   2.90329959e-02,\n",
       "        -5.43098012e-03,  -1.14447571e-01,  -2.14308649e-02,\n",
       "         9.65253487e-02,  -8.23709369e-02,  -1.73156172e-01,\n",
       "         2.32566539e-02,   2.55151419e-03,   9.80864018e-02,\n",
       "        -1.78243697e-01,  -1.49945375e-02,   6.76475465e-02,\n",
       "         1.07409090e-01,   5.67482300e-02,  -1.56656485e-02,\n",
       "         2.79527251e-02,  -4.69184853e-02,   1.28213480e-01,\n",
       "        -9.32300836e-02,   6.10402320e-04,  -4.29119542e-02,\n",
       "        -6.36665802e-03,  -4.21965867e-02,   4.33073603e-02,\n",
       "         6.72652647e-02,   2.23526452e-02,   4.31573503e-02,\n",
       "         3.43929976e-02,  -7.66830519e-02,  -4.00870666e-02,\n",
       "         3.30801234e-02,  -3.01707461e-02,  -8.15262496e-02,\n",
       "         7.08192810e-02,  -9.26138982e-02,  -6.75996989e-02,\n",
       "         7.88059272e-03,  -1.98200271e-02,  -1.99262857e-01,\n",
       "        -1.86466053e-01,   4.03886661e-02,  -8.97997394e-02,\n",
       "         3.22730988e-02,   1.31101578e-01,  -1.09601365e-02,\n",
       "         7.58855790e-02,  -9.07023624e-03,  -5.18578812e-02,\n",
       "         1.12119995e-01,  -8.12914222e-03,  -3.38170342e-02,\n",
       "        -9.01480541e-02,  -5.69725037e-02,   8.49739555e-03,\n",
       "         2.59825718e-02,   1.24760464e-01,  -3.20391133e-02,\n",
       "         8.30915123e-02,   7.96777010e-02,  -4.57263812e-02,\n",
       "         1.51204048e-02,   7.61255175e-02,   4.95562367e-02,\n",
       "        -5.19720539e-02,   3.63458693e-02,  -4.79294062e-02,\n",
       "         4.53895256e-02,   2.21041143e-02,   1.16093591e-01,\n",
       "        -7.01349601e-02,   4.52102125e-02,  -2.61992938e-03,\n",
       "         3.81872989e-02,  -3.19106281e-02,  -7.49604851e-02,\n",
       "        -1.49373086e-02,   2.79787369e-02,  -1.03231696e-02,\n",
       "         9.14983824e-02,  -3.58918775e-03,   5.56202792e-02,\n",
       "        -5.38781434e-02,  -1.10785589e-02,  -1.83128938e-02,\n",
       "        -1.79675911e-02,  -4.49799523e-02,   1.03089251e-01,\n",
       "        -1.51833119e-02,   3.63299847e-02,  -2.15609111e-02,\n",
       "        -9.57430005e-02,  -4.32135798e-02,   1.00862365e-02,\n",
       "        -1.27213905e-02,   6.23016758e-03,  -1.63204630e-03,\n",
       "        -6.60525560e-02,  -2.83950998e-04,  -2.07037646e-02,\n",
       "        -1.21290982e-02,   6.55819252e-02,   5.84300421e-02,\n",
       "         9.94559750e-03,  -1.04749374e-01,   1.59412641e-02,\n",
       "        -3.30231935e-02,  -4.08307500e-02,  -6.23369776e-03,\n",
       "        -2.27735681e-03,   9.54739079e-02,   6.90527027e-03,\n",
       "        -8.59968364e-02,   6.74364865e-02,  -1.00668101e-02,\n",
       "         3.66066471e-02,  -2.56750155e-02,  -3.32948682e-03,\n",
       "        -5.76843433e-02,  -1.41730055e-01,  -5.13792597e-02,\n",
       "         1.76766198e-02,  -1.64140407e-02,   4.61266413e-02,\n",
       "         2.22204309e-02,   1.24150254e-01,   1.85276523e-01,\n",
       "         9.27937124e-03,   3.25459391e-02,  -1.19409911e-01,\n",
       "         3.48396748e-02,   1.60069913e-02,  -4.28065062e-02,\n",
       "         4.74685654e-02,   8.53237975e-03,   2.78203897e-02,\n",
       "         8.81002098e-03,   2.67470023e-03,   7.46859387e-02,\n",
       "         3.96451466e-02,   4.59050760e-02,   1.45570695e-01,\n",
       "         1.97918564e-02,  -6.10547066e-02,   2.84774248e-02,\n",
       "        -1.88701358e-02,  -1.91202424e-02,   1.70453247e-02,\n",
       "        -6.26782477e-02,  -3.97139825e-02,   1.08168758e-01,\n",
       "         2.76686754e-02,  -1.23113900e-01,  -8.01577326e-03,\n",
       "         1.25835175e-02,   3.39015499e-02,   2.97406577e-02,\n",
       "         1.34538993e-01,  -1.02463461e-01,  -1.92660931e-02,\n",
       "         1.40661389e-01,   1.21968374e-01,   5.47416955e-02,\n",
       "         2.92322319e-02,   2.96813957e-02,   2.00752579e-02,\n",
       "        -4.75969799e-02,  -1.63475156e-01,   3.66086587e-02,\n",
       "        -4.53663468e-02,  -5.64101487e-02,   2.45775352e-03,\n",
       "         2.27523595e-02,   2.28450075e-02,  -4.76916581e-02,\n",
       "         5.05441800e-02,   2.07732599e-02,   1.02421939e-02,\n",
       "         7.48222396e-02,  -2.61904020e-02,   9.53938216e-02,\n",
       "         3.29866186e-02,  -5.47599569e-02,   1.56301935e-03,\n",
       "         3.82115431e-02,   5.82279451e-03,   2.46687233e-02,\n",
       "         1.25354066e-01,  -5.46214618e-02,   8.38535726e-02,\n",
       "         6.35111006e-03,   4.82186396e-03,   1.17905661e-01,\n",
       "        -7.98089877e-02,  -7.40831643e-02,   6.66028485e-02,\n",
       "        -9.43199173e-02,   4.62392420e-02,   4.39159870e-02,\n",
       "         6.27595857e-02,  -6.83563799e-02,  -9.30626132e-03,\n",
       "        -1.05103524e-02,   7.91137218e-02,   1.57165211e-02,\n",
       "         5.56945875e-02,   5.84712811e-03,   1.50026958e-02,\n",
       "        -1.13853268e-01,  -9.55067500e-02,  -1.03334278e-01,\n",
       "        -1.96146518e-02,  -1.49616292e-02,   1.81347539e-04,\n",
       "        -8.84015858e-02,  -7.81434327e-02,   3.96284014e-02,\n",
       "         1.38468938e-02,   5.62418764e-03,  -3.11080720e-02,\n",
       "        -9.41440091e-03,  -8.50022398e-03,   5.74442819e-02,\n",
       "         8.14076960e-02,  -3.62089439e-03,   4.92821969e-02,\n",
       "         4.57677990e-02,  -1.36096962e-02,   2.42830925e-02,\n",
       "         2.33129002e-02,   2.35944055e-02,  -6.97415248e-02,\n",
       "         2.54962910e-02,   4.80165146e-02,  -1.90509465e-02,\n",
       "        -5.99239469e-02,  -1.28424661e-02,  -4.36503738e-02,\n",
       "         6.83674738e-02,   5.30378073e-02,   1.44694611e-01,\n",
       "        -6.23575225e-02,  -8.90865773e-02,  -4.02439237e-02,\n",
       "        -2.06628218e-02,   1.61948055e-01,   3.96691896e-02,\n",
       "         2.96270549e-02,  -2.28325166e-02,  -7.90856555e-02,\n",
       "         1.30321041e-01,   8.46751109e-02,   3.13368477e-02,\n",
       "         7.32486621e-02,   3.84858213e-02,  -4.30912636e-02,\n",
       "        -4.26176451e-02,  -1.30430991e-02,  -3.71295623e-02,\n",
       "        -2.88008824e-02,   1.94201946e-01,  -3.86790894e-02,\n",
       "         2.33671609e-02,  -1.37227207e-01,  -3.79416766e-03,\n",
       "        -5.85510209e-03,  -4.04221453e-02,  -4.05625477e-02,\n",
       "         8.39164704e-02,   2.89317835e-02,  -1.33754998e-01,\n",
       "         8.75207782e-02,  -1.23293195e-02,  -1.91334647e-03,\n",
       "        -2.06910726e-02,   2.17145924e-02,   1.91025659e-02,\n",
       "         4.41481546e-02,   3.93687375e-02,   4.15013731e-03,\n",
       "        -2.09624525e-02,   4.32396233e-02,  -3.31139155e-02,\n",
       "        -5.52762579e-03,   1.78534184e-02,  -2.60880664e-02,\n",
       "        -4.83804122e-02,  -1.26382485e-02,   3.75093892e-02,\n",
       "        -3.27125266e-02,  -2.75921486e-02,   9.25027858e-03,\n",
       "        -8.73976722e-02,  -5.47009967e-02,   1.76140755e-01,\n",
       "         3.52531411e-02,  -9.92835835e-02,  -1.31610468e-01,\n",
       "         2.52186414e-02,   1.79740526e-02,   1.35655347e-02,\n",
       "         1.43760871e-02,   7.21597597e-02,  -1.20249540e-01,\n",
       "        -1.48497624e-02,   5.28645441e-02,   1.10533953e-01,\n",
       "         1.79703999e-02,  -3.79603952e-02,  -4.22572792e-02,\n",
       "        -3.71766579e-03,   2.28689401e-03,   3.95486467e-02,\n",
       "         7.63300359e-02,  -1.06703021e-01,  -1.37694748e-02,\n",
       "         5.74833974e-02,   4.05516801e-03,  -2.95937620e-03,\n",
       "         4.10623327e-02,  -9.21738073e-02,  -4.86463383e-02,\n",
       "         7.71367550e-02,  -1.69994030e-02,  -2.53908932e-02,\n",
       "        -1.76934730e-02,  -7.74437264e-02,   3.73487286e-02,\n",
       "         1.68459993e-02,  -9.14084837e-02,   3.14687192e-02,\n",
       "        -6.34455457e-02,   1.00618660e-01,  -5.90840690e-02,\n",
       "         4.57061082e-02,   1.01604667e-02,   1.28325403e-01,\n",
       "        -5.22046126e-02,   7.27732405e-02,   9.25005898e-02,\n",
       "         8.30214377e-03,  -2.59517003e-02,   3.36906477e-03,\n",
       "        -4.34864964e-03,   4.87807766e-02,  -1.79915786e-01,\n",
       "         3.32015902e-02,  -7.32086450e-02,   3.60612161e-02,\n",
       "         5.26994001e-03,  -2.06268951e-02,   1.12699740e-01,\n",
       "        -2.70597753e-03,  -3.36667076e-02,  -3.38765979e-02,\n",
       "        -1.85481347e-02,   6.63165152e-02,  -6.76674908e-03,\n",
       "         3.87451635e-03,  -1.57038763e-01,  -1.26600474e-01,\n",
       "        -7.97324106e-02,   3.62651609e-02,  -8.93671904e-03,\n",
       "        -5.77721335e-02,  -7.68194646e-02,   2.12427340e-02,\n",
       "         8.55780169e-02,   5.61571196e-02,   3.44274677e-02,\n",
       "        -1.72613971e-02,   5.58637530e-02,   6.80561513e-02,\n",
       "         1.84967928e-02,  -2.68534236e-02,  -5.86445257e-02,\n",
       "         2.81999949e-02,   6.23091124e-02,   2.12934725e-02,\n",
       "         9.12088454e-02,  -1.33634016e-01,   3.81767750e-02,\n",
       "         3.48444358e-02,   5.06025814e-02,   4.94861491e-02,\n",
       "         1.15964964e-01,   7.65616372e-02,  -8.46867114e-02,\n",
       "        -2.56883390e-02,   1.02719136e-01,  -4.13589738e-03,\n",
       "        -2.56452523e-02,  -1.69742741e-02,  -2.81852428e-02,\n",
       "        -9.11602601e-02,   2.96309032e-02,  -1.02624997e-01,\n",
       "         7.40024168e-03,   1.65557619e-02,  -2.45338604e-02,\n",
       "        -1.17890902e-01,  -2.52864826e-02,  -8.41538832e-02,\n",
       "        -7.75295570e-02,   3.98649536e-02,   4.94574308e-02,\n",
       "         9.61793179e-04,   5.34676462e-02,   1.56366974e-01,\n",
       "        -7.80105740e-02,  -4.36066911e-02,   5.69092929e-02,\n",
       "         4.31900136e-02,  -6.96267709e-02,  -1.04108928e-02,\n",
       "         2.17547510e-02,  -9.94285941e-03,  -2.76953783e-02,\n",
       "         8.07660222e-02,   9.63634700e-02,   5.53117283e-02,\n",
       "        -6.52518868e-02,   4.00099754e-02,  -5.84285036e-02,\n",
       "        -2.56264769e-03,  -2.94849575e-02,  -2.51241885e-02,\n",
       "         3.10615525e-02,   7.64318779e-02,  -2.17061453e-02,\n",
       "         1.81497373e-02,   3.92123312e-02,   2.76888590e-02,\n",
       "         3.91689092e-02,  -5.27603477e-02,   1.22981116e-01,\n",
       "         5.22178449e-02,   5.89990318e-02,   1.01681035e-02,\n",
       "         6.64729625e-02,   4.10759822e-02,   3.02489623e-02,\n",
       "         9.50959325e-02,   1.49612082e-02,  -1.48022279e-01,\n",
       "         1.05787233e-01,  -3.81440111e-02,   4.66250256e-02,\n",
       "         6.62510917e-02,   6.09028526e-02,  -4.02918868e-02,\n",
       "         9.25607905e-02,  -6.48859665e-02,  -1.35482952e-01,\n",
       "         1.51395714e-02,  -6.87583312e-02,   1.20740114e-02,\n",
       "        -5.99207096e-02,   3.04517858e-02,   2.99817454e-02,\n",
       "        -6.68322295e-02,  -6.17168881e-02,  -2.07524821e-02,\n",
       "        -1.50126785e-01,   8.15106779e-02,  -2.16601193e-02,\n",
       "         7.22772395e-03,  -6.21988103e-02,  -2.54315007e-02,\n",
       "         9.55514684e-02,   2.18628850e-02,  -2.30988357e-02,\n",
       "         4.42302749e-02,   3.50625031e-02,  -1.38994604e-02,\n",
       "        -1.10227928e-01,  -7.70911500e-02,   1.20042019e-01,\n",
       "        -1.01550832e-01,  -7.29815289e-02,  -1.69299599e-02,\n",
       "         2.39854329e-03,  -1.59807550e-03,   5.82459942e-02,\n",
       "        -4.90854234e-02,  -3.48853320e-02,  -2.92216334e-02,\n",
       "        -4.52678464e-02,   6.98715970e-02,   3.14552635e-02,\n",
       "         3.44348811e-02,   5.93056604e-02,  -3.81521396e-02,\n",
       "         4.91837300e-02,   7.54177338e-03,  -2.63246652e-02,\n",
       "         5.86738400e-02,  -5.04541658e-02,  -8.82639736e-02,\n",
       "         8.79784673e-02,   4.26625013e-02,   6.42572343e-02,\n",
       "         1.61026008e-02,   6.52121007e-03,  -2.74103694e-02,\n",
       "         4.93352721e-03,   4.38257232e-02,  -6.14512190e-02,\n",
       "         1.31153911e-01,   5.05853146e-02,  -1.14456691e-01,\n",
       "        -5.43775000e-02,   1.61384761e-01,  -1.11732773e-01,\n",
       "        -2.17607189e-02,  -6.97092935e-02,   1.06261363e-02,\n",
       "        -6.92907944e-02,  -5.74194677e-02,   5.82792424e-02,\n",
       "        -5.69625869e-02,   1.37002751e-01,   4.59370539e-02,\n",
       "        -1.12145953e-02,   4.30621356e-02,  -5.35798371e-02,\n",
       "         1.52644631e-03,  -9.07993466e-02,  -7.56890997e-02,\n",
       "         8.58703405e-02,   5.15090786e-02,   2.11906247e-03,\n",
       "         5.18877199e-03,   9.39424988e-03,  -1.92852356e-02,\n",
       "        -4.85813543e-02,  -4.78060208e-02,   4.46362533e-02,\n",
       "         3.31387594e-02,  -4.95980354e-03,   1.17088351e-02,\n",
       "        -8.52207914e-02,   2.44174153e-02,   5.09181730e-02,\n",
       "         1.70764178e-02,  -1.49676437e-02,  -3.64723131e-02,\n",
       "         4.52402756e-02,   6.56954944e-02,  -1.36063360e-02,\n",
       "         8.15650523e-02,  -2.38218401e-02,  -3.01242210e-02,\n",
       "        -1.92704163e-02,   2.45141406e-02,  -2.91793253e-02,\n",
       "         3.95579785e-02,   5.27153164e-02,  -1.84912290e-02,\n",
       "         3.94477025e-02,   1.31131127e-01,   5.78168146e-02,\n",
       "        -1.32876202e-01,   1.51480604e-02,  -2.91022211e-02,\n",
       "        -5.20155579e-03,   1.38493344e-01,   2.90229004e-02,\n",
       "         1.35978237e-02,   5.94120063e-02,  -9.10976902e-02,\n",
       "         9.43460129e-03], dtype=float32)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchW2V['bien']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "this method returns the top n similar ones. This is an interesting feature. Let's try it on some words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-08-24 11:30:16,387 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'bien', 0.5377739667892456),\n",
       " (u'mieux', 0.4622610807418823),\n",
       " (u'pourquoi', 0.3916780352592468),\n",
       " (u'peur', 0.3831149637699127),\n",
       " (u'pourtant', 0.3812871277332306),\n",
       " (u'foutu', 0.3798130452632904),\n",
       " (u'compliqu\\xe9', 0.37696385383605957),\n",
       " (u'intentionn\\xe9s', 0.36648163199424744),\n",
       " (u'comprendre', 0.36104655265808105),\n",
       " (u'trop', 0.3609797954559326)]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frenchW2V.most_similar('mal')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We first have to reduce their dimension to 2 using t-SNE. Then, using an interactive visualization tool such as Bokeh,\n",
    "we can map them directly on 2D plane and interact with them."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's now get to the sentiment classification part. As for now, we have a word2vec model that converts each word from the corpus into a high dimensional vector. This seems to work fine according to the similarity tests and the bokeh chart.\n",
    "In order to classify tweets, we have to turn them into vectors as well. This task is almost done. Since we know the vector representation of each word composing a tweet, we have to \"combine\" these vectors together and get a new one that represents the tweet as a whole.\n",
    "A first approach consists in averaging the word vectors together. But a slightly better solution is to compute a weighted average where each weight gives the importance of the word with respect to the corpus. Such a weight could the tf-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 15482\n"
     ]
    }
   ],
   "source": [
    "print 'building tf-idf matrix ...'\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in x_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print 'vocab size :', len(tfidf)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now let's define a function that, given a list of tweet tokens, creates an averaged tweet vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += frenchW2V[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we convert x_train and and x_test into list of vectors using this function. We also scale each column to have zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 458843/458843 [01:26<00:00, 5297.22it/s]\n",
      "100%|ââââââââââ| 114711/114711 [00:28<00:00, 3993.67it/s]\n"
     ]
    }
   ],
   "source": [
    "n_dim=1000\n",
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2vfr = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2vfr = scale(train_vecs_w2vfr)\n",
    "\n",
    "test_vecs_w2vfr = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2vfr = scale(test_vecs_w2vfr)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We should now be ready to feed these vectors into a neural network classifier. In fact, using Keras is very easy to define layers and activation functions.\n",
    "A basic 2-Layer  architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving numpy array\n",
    "np.savetxt('trainw2vfr.txt',train_vecs_w2vfr , fmt='%10.8f')\n",
    "np.savetxt('testw2vfr.txt',test_vecs_w2vfr , fmt='%10.8f')\n",
    "np.savetxt('y_trainfr.txt',y_train , fmt='%d')\n",
    "np.savetxt('y_testfr.txt',y_test , fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model on the Train and Test Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "from keras.models import Sequential\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "import keras\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from keras.layers import Activation, Dense\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing numpy arrays\n",
    "trainvecs = np.loadtxt('trainw2vfr.txt', dtype=float)\n",
    "testvecs = np.loadtxt('testw2vfr.txt', dtype=float)\n",
    "y_trainvecs = np.loadtxt('y_trainfr.txt', dtype=int)\n",
    "y_testvecs = np.loadtxt('y_testfr.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model works better than the model below\n",
    "# to define that my model is a sequence of layers\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph/ANN', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model = Sequential()\n",
    "# defining my first hidden layer, output size is 100\n",
    "model.add(Dense(100, activation='sigmoid', input_dim=1000))\n",
    "# defining my second hidden layer, output size is 50\n",
    "model.add(Dense(50, activation='relu', input_dim=100))\n",
    "# defining my third hidden layer, output size is 32\n",
    "model.add(Dense(32, activation='relu', input_dim=50))\n",
    "# defining my fourth hidden layer, output size is 16\n",
    "model.add(Dense(16, activation='relu', input_dim=32))\n",
    "# defining my output layer, output size is 1\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# I just need to compile the model and I'll be ready to train it. \n",
    "#When I compile the model, I declare the loss function and the optimizer (SGD, Adam, etc.).\n",
    "# Here I have used 'Adam' optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# to define that my model is a sequence of layers\n",
    "model = Sequential()\n",
    "# defining my first hidden layer, output size is 32\n",
    "model.add(Dense(500, activation='sigmoid', input_dim=1000))\n",
    "# defining my second hidden layer, output size is 1\n",
    "model.add(Dense(250, activation='relu', input_dim=500))\n",
    "model.add(Dense(125, activation='relu', input_dim=250))\n",
    "model.add(Dense(64, activation='relu', input_dim=125))\n",
    "model.add(Dense(32, activation='relu', input_dim=64))\n",
    "model.add(Dense(16, activation='relu', input_dim=32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# I just need to compile the model and I'll be ready to train it. \n",
    "#When I compile the model, I declare the loss function and the optimizer (SGD, Adam, etc.).\n",
    "# Here I have used 'Adam' optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/125\n",
      "66s - loss: -1.3933e+00 - acc: 0.8168\n",
      "Epoch 2/125\n",
      "64s - loss: -1.6835e+00 - acc: 0.8268\n",
      "Epoch 3/125\n",
      "63s - loss: -1.7801e+00 - acc: 0.8294\n",
      "Epoch 4/125\n",
      "63s - loss: -1.8223e+00 - acc: 0.8306\n",
      "Epoch 5/125\n",
      "64s - loss: -1.8388e+00 - acc: 0.8306\n",
      "Epoch 6/125\n",
      "64s - loss: -1.8607e+00 - acc: 0.8315\n",
      "Epoch 7/125\n",
      "63s - loss: -1.8763e+00 - acc: 0.8329\n",
      "Epoch 8/125\n",
      "63s - loss: -1.8928e+00 - acc: 0.8323\n",
      "Epoch 9/125\n",
      "63s - loss: -1.9105e+00 - acc: 0.8324\n",
      "Epoch 10/125\n",
      "64s - loss: -1.9109e+00 - acc: 0.8325\n",
      "Epoch 11/125\n",
      "63s - loss: -1.9189e+00 - acc: 0.8324\n",
      "Epoch 12/125\n",
      "64s - loss: -1.9071e+00 - acc: 0.8316\n",
      "Epoch 13/125\n",
      "63s - loss: -1.9261e+00 - acc: 0.8329\n",
      "Epoch 14/125\n",
      "64s - loss: -1.9341e+00 - acc: 0.8329\n",
      "Epoch 15/125\n",
      "64s - loss: -1.9453e+00 - acc: 0.8334\n",
      "Epoch 16/125\n",
      "64s - loss: -1.9413e+00 - acc: 0.8327\n",
      "Epoch 17/125\n",
      "63s - loss: -1.9504e+00 - acc: 0.8328\n",
      "Epoch 18/125\n",
      "64s - loss: -1.9443e+00 - acc: 0.8327\n",
      "Epoch 19/125\n",
      "63s - loss: -1.9611e+00 - acc: 0.8329\n",
      "Epoch 20/125\n",
      "63s - loss: -1.9593e+00 - acc: 0.8333\n",
      "Epoch 21/125\n",
      "64s - loss: -1.9458e+00 - acc: 0.8325\n",
      "Epoch 22/125\n",
      "63s - loss: -1.9679e+00 - acc: 0.8332\n",
      "Epoch 23/125\n",
      "63s - loss: -1.9805e+00 - acc: 0.8336\n",
      "Epoch 24/125\n",
      "63s - loss: -1.9744e+00 - acc: 0.8336\n",
      "Epoch 25/125\n",
      "63s - loss: -1.9799e+00 - acc: 0.8333\n",
      "Epoch 26/125\n",
      "63s - loss: -1.9839e+00 - acc: 0.8336\n",
      "Epoch 27/125\n",
      "64s - loss: -1.9902e+00 - acc: 0.8338\n",
      "Epoch 28/125\n",
      "64s - loss: -1.9987e+00 - acc: 0.8345\n",
      "Epoch 29/125\n",
      "63s - loss: -1.9911e+00 - acc: 0.8335\n",
      "Epoch 30/125\n",
      "63s - loss: -1.9822e+00 - acc: 0.8329\n",
      "Epoch 31/125\n",
      "64s - loss: -2.0061e+00 - acc: 0.8337\n",
      "Epoch 32/125\n",
      "64s - loss: -2.0002e+00 - acc: 0.8336\n",
      "Epoch 33/125\n",
      "63s - loss: -2.0100e+00 - acc: 0.8348\n",
      "Epoch 34/125\n",
      "64s - loss: -2.0252e+00 - acc: 0.8344\n",
      "Epoch 35/125\n",
      "64s - loss: -2.0344e+00 - acc: 0.8352\n",
      "Epoch 36/125\n",
      "63s - loss: -2.0188e+00 - acc: 0.8346\n",
      "Epoch 37/125\n",
      "63s - loss: -2.0251e+00 - acc: 0.8350\n",
      "Epoch 38/125\n",
      "63s - loss: -2.0149e+00 - acc: 0.8336\n",
      "Epoch 39/125\n",
      "63s - loss: -2.0210e+00 - acc: 0.8342\n",
      "Epoch 40/125\n",
      "63s - loss: -2.0201e+00 - acc: 0.8340\n",
      "Epoch 41/125\n",
      "63s - loss: -2.0266e+00 - acc: 0.8343\n",
      "Epoch 42/125\n",
      "64s - loss: -2.0229e+00 - acc: 0.8342\n",
      "Epoch 43/125\n",
      "64s - loss: -2.0329e+00 - acc: 0.8348\n",
      "Epoch 44/125\n",
      "64s - loss: -2.0239e+00 - acc: 0.8341\n",
      "Epoch 45/125\n",
      "64s - loss: -2.0231e+00 - acc: 0.8343\n",
      "Epoch 46/125\n",
      "63s - loss: -2.0304e+00 - acc: 0.8338\n",
      "Epoch 47/125\n",
      "63s - loss: -2.0300e+00 - acc: 0.8339\n",
      "Epoch 48/125\n",
      "64s - loss: -2.0470e+00 - acc: 0.8349\n",
      "Epoch 49/125\n",
      "63s - loss: -2.0479e+00 - acc: 0.8349\n",
      "Epoch 50/125\n",
      "63s - loss: -2.0571e+00 - acc: 0.8348\n",
      "Epoch 51/125\n",
      "63s - loss: -2.0692e+00 - acc: 0.8353\n",
      "Epoch 52/125\n",
      "63s - loss: -2.0704e+00 - acc: 0.8357\n",
      "Epoch 53/125\n",
      "63s - loss: -2.0704e+00 - acc: 0.8359\n",
      "Epoch 54/125\n",
      "63s - loss: -2.0664e+00 - acc: 0.8357\n",
      "Epoch 55/125\n",
      "63s - loss: -2.0736e+00 - acc: 0.8353\n",
      "Epoch 56/125\n",
      "63s - loss: -2.0824e+00 - acc: 0.8356\n",
      "Epoch 57/125\n",
      "64s - loss: -2.0784e+00 - acc: 0.8361\n",
      "Epoch 58/125\n",
      "64s - loss: -2.0785e+00 - acc: 0.8361\n",
      "Epoch 59/125\n",
      "64s - loss: -2.0738e+00 - acc: 0.8355\n",
      "Epoch 60/125\n",
      "64s - loss: -2.0836e+00 - acc: 0.8360\n",
      "Epoch 61/125\n",
      "63s - loss: -2.0850e+00 - acc: 0.8362\n",
      "Epoch 62/125\n",
      "64s - loss: -2.0811e+00 - acc: 0.8355\n",
      "Epoch 63/125\n",
      "63s - loss: -2.0835e+00 - acc: 0.8358\n",
      "Epoch 64/125\n",
      "63s - loss: -2.0943e+00 - acc: 0.8357\n",
      "Epoch 65/125\n",
      "63s - loss: -2.0900e+00 - acc: 0.8362\n",
      "Epoch 66/125\n",
      "64s - loss: -2.0907e+00 - acc: 0.8360\n",
      "Epoch 67/125\n",
      "64s - loss: -2.0941e+00 - acc: 0.8362\n",
      "Epoch 68/125\n",
      "63s - loss: -2.0968e+00 - acc: 0.8359\n",
      "Epoch 69/125\n",
      "64s - loss: -2.0929e+00 - acc: 0.8362\n",
      "Epoch 70/125\n",
      "63s - loss: -2.0975e+00 - acc: 0.8361\n",
      "Epoch 71/125\n",
      "64s - loss: -2.1015e+00 - acc: 0.8358\n",
      "Epoch 72/125\n",
      "64s - loss: -2.1095e+00 - acc: 0.8358\n",
      "Epoch 73/125\n",
      "63s - loss: -2.1136e+00 - acc: 0.8367\n",
      "Epoch 74/125\n",
      "63s - loss: -2.1157e+00 - acc: 0.8365\n",
      "Epoch 75/125\n",
      "63s - loss: -2.1129e+00 - acc: 0.8366\n",
      "Epoch 76/125\n",
      "63s - loss: -2.1223e+00 - acc: 0.8370\n",
      "Epoch 77/125\n",
      "63s - loss: -2.1192e+00 - acc: 0.8365\n",
      "Epoch 78/125\n",
      "64s - loss: -2.1315e+00 - acc: 0.8368\n",
      "Epoch 79/125\n",
      "63s - loss: -2.1269e+00 - acc: 0.8367\n",
      "Epoch 80/125\n",
      "63s - loss: -2.1273e+00 - acc: 0.8372\n",
      "Epoch 81/125\n",
      "64s - loss: -2.1211e+00 - acc: 0.8367\n",
      "Epoch 82/125\n",
      "64s - loss: -2.1212e+00 - acc: 0.8365\n",
      "Epoch 83/125\n",
      "64s - loss: -2.1284e+00 - acc: 0.8368\n",
      "Epoch 84/125\n",
      "63s - loss: -2.1243e+00 - acc: 0.8363\n",
      "Epoch 85/125\n",
      "64s - loss: -2.1210e+00 - acc: 0.8368\n",
      "Epoch 86/125\n",
      "64s - loss: -2.1207e+00 - acc: 0.8363\n",
      "Epoch 87/125\n",
      "64s - loss: -2.1299e+00 - acc: 0.8365\n",
      "Epoch 88/125\n",
      "63s - loss: -2.1207e+00 - acc: 0.8366\n",
      "Epoch 89/125\n",
      "63s - loss: -2.1308e+00 - acc: 0.8365\n",
      "Epoch 90/125\n",
      "63s - loss: -2.1365e+00 - acc: 0.8374\n",
      "Epoch 91/125\n",
      "63s - loss: -2.1411e+00 - acc: 0.8375\n",
      "Epoch 92/125\n",
      "63s - loss: -2.1495e+00 - acc: 0.8373\n",
      "Epoch 93/125\n",
      "64s - loss: -2.1463e+00 - acc: 0.8369\n",
      "Epoch 94/125\n",
      "64s - loss: -2.1481e+00 - acc: 0.8368\n",
      "Epoch 95/125\n",
      "63s - loss: -2.1400e+00 - acc: 0.8367\n",
      "Epoch 96/125\n",
      "64s - loss: -2.1472e+00 - acc: 0.8366\n",
      "Epoch 97/125\n",
      "64s - loss: -2.1362e+00 - acc: 0.8366\n",
      "Epoch 98/125\n",
      "63s - loss: -2.1373e+00 - acc: 0.8365\n",
      "Epoch 99/125\n",
      "63s - loss: -2.1309e+00 - acc: 0.8362\n",
      "Epoch 100/125\n",
      "63s - loss: -2.1246e+00 - acc: 0.8365\n",
      "Epoch 101/125\n",
      "64s - loss: -2.1427e+00 - acc: 0.8372\n",
      "Epoch 102/125\n",
      "63s - loss: -2.1395e+00 - acc: 0.8365\n",
      "Epoch 103/125\n",
      "64s - loss: -2.1498e+00 - acc: 0.8373\n",
      "Epoch 104/125\n",
      "63s - loss: -2.1475e+00 - acc: 0.8373\n",
      "Epoch 105/125\n",
      "63s - loss: -2.1522e+00 - acc: 0.8373\n",
      "Epoch 106/125\n",
      "64s - loss: -2.1478e+00 - acc: 0.8374\n",
      "Epoch 107/125\n",
      "64s - loss: -2.1528e+00 - acc: 0.8373\n",
      "Epoch 108/125\n",
      "64s - loss: -2.1504e+00 - acc: 0.8371\n",
      "Epoch 109/125\n",
      "64s - loss: -2.1581e+00 - acc: 0.8375\n",
      "Epoch 110/125\n",
      "64s - loss: -2.1483e+00 - acc: 0.8372\n",
      "Epoch 111/125\n",
      "64s - loss: -2.1654e+00 - acc: 0.8371\n",
      "Epoch 112/125\n",
      "63s - loss: -2.1689e+00 - acc: 0.8376\n",
      "Epoch 113/125\n",
      "63s - loss: -2.1631e+00 - acc: 0.8375\n",
      "Epoch 114/125\n",
      "64s - loss: -2.1712e+00 - acc: 0.8375\n",
      "Epoch 115/125\n",
      "64s - loss: -2.1804e+00 - acc: 0.8379\n",
      "Epoch 116/125\n",
      "63s - loss: -2.1711e+00 - acc: 0.8376\n",
      "Epoch 117/125\n",
      "63s - loss: -2.1859e+00 - acc: 0.8381\n",
      "Epoch 118/125\n",
      "63s - loss: -2.1840e+00 - acc: 0.8381\n",
      "Epoch 119/125\n",
      "63s - loss: -2.1786e+00 - acc: 0.8378\n",
      "Epoch 120/125\n",
      "63s - loss: -2.1822e+00 - acc: 0.8382\n",
      "Epoch 121/125\n",
      "63s - loss: -2.1782e+00 - acc: 0.8375\n",
      "Epoch 122/125\n",
      "63s - loss: -2.1806e+00 - acc: 0.8375\n",
      "Epoch 123/125\n",
      "64s - loss: -2.1902e+00 - acc: 0.8384\n",
      "Epoch 124/125\n",
      "63s - loss: -2.1973e+00 - acc: 0.8384\n",
      "Epoch 125/125\n",
      "63s - loss: -2.1956e+00 - acc: 0.8383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3e502c98d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To fit the model, all I have to do is declare the batch size and number of epochs to train for, \n",
    "# then pass in my training data\n",
    "model.fit(trainvecs, y_trainvecs, epochs=125, batch_size=32, verbose=2,callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model:\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "model.save('ann_model_fr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing ANN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling/ importing the model\n",
    "model = load_model('ann_model_fr.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that the model is trained, let's evaluate it on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.831637767968\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(testvecs, y_testvecs, batch_size=128, verbose=2)\n",
    "print score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for the sentiment analysis problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 66274"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad dataset to a maximum review length in words\n",
    "max_words = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=trainvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=testvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_trainvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_testvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 140, 32)           2120768   \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 140, 32)           3104      \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 140, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_19 (MaxPooling (None, 70, 32)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_20 (MaxPooling (None, 35, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 1120)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 250)               280250    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 125)               31375     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 1)                 126       \n",
      "=================================================================\n",
      "Total params: 2,438,727\n",
      "Trainable params: 2,438,727\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph/CNN', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "#model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='sigmoid'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='tanh'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(125, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 458843 samples, validate on 114711 samples\n",
      "Epoch 1/100\n",
      "45s - loss: -7.3660e-01 - acc: 0.7792 - val_loss: -1.1727e+00 - val_acc: 0.8002\n",
      "Epoch 2/100\n",
      "45s - loss: -1.3613e+00 - acc: 0.8007 - val_loss: -1.3425e+00 - val_acc: 0.7991\n",
      "Epoch 3/100\n",
      "45s - loss: -1.5244e+00 - acc: 0.8062 - val_loss: -1.4022e+00 - val_acc: 0.7946\n",
      "Epoch 4/100\n",
      "45s - loss: -1.6254e+00 - acc: 0.8094 - val_loss: -1.4274e+00 - val_acc: 0.8102\n",
      "Epoch 5/100\n",
      "45s - loss: -1.6942e+00 - acc: 0.8113 - val_loss: -1.4688e+00 - val_acc: 0.8051\n",
      "Epoch 6/100\n",
      "45s - loss: -1.7595e+00 - acc: 0.8135 - val_loss: -1.4741e+00 - val_acc: 0.7990\n",
      "Epoch 7/100\n",
      "45s - loss: -1.8042e+00 - acc: 0.8147 - val_loss: -1.4671e+00 - val_acc: 0.8009\n",
      "Epoch 8/100\n",
      "45s - loss: -1.8474e+00 - acc: 0.8160 - val_loss: -1.4759e+00 - val_acc: 0.8039\n",
      "Epoch 9/100\n",
      "45s - loss: -1.8848e+00 - acc: 0.8174 - val_loss: -1.4647e+00 - val_acc: 0.8082\n",
      "Epoch 10/100\n",
      "46s - loss: -1.9122e+00 - acc: 0.8184 - val_loss: -1.4524e+00 - val_acc: 0.8015\n",
      "Epoch 11/100\n",
      "46s - loss: -1.9402e+00 - acc: 0.8195 - val_loss: -1.4249e+00 - val_acc: 0.7995\n",
      "Epoch 12/100\n",
      "45s - loss: -1.9623e+00 - acc: 0.8205 - val_loss: -1.3854e+00 - val_acc: 0.7974\n",
      "Epoch 13/100\n",
      "46s - loss: -1.9831e+00 - acc: 0.8214 - val_loss: -1.3761e+00 - val_acc: 0.8067\n",
      "Epoch 14/100\n",
      "45s - loss: -2.0016e+00 - acc: 0.8223 - val_loss: -1.3671e+00 - val_acc: 0.8051\n",
      "Epoch 15/100\n",
      "45s - loss: -2.0137e+00 - acc: 0.8229 - val_loss: -1.3480e+00 - val_acc: 0.8130\n",
      "Epoch 16/100\n",
      "46s - loss: -2.0240e+00 - acc: 0.8236 - val_loss: -1.3212e+00 - val_acc: 0.8062\n",
      "Epoch 17/100\n",
      "45s - loss: -2.0365e+00 - acc: 0.8241 - val_loss: -1.2969e+00 - val_acc: 0.8127\n",
      "Epoch 18/100\n",
      "45s - loss: -2.0459e+00 - acc: 0.8246 - val_loss: -1.3131e+00 - val_acc: 0.8110\n",
      "Epoch 19/100\n",
      "45s - loss: -2.0480e+00 - acc: 0.8249 - val_loss: -1.2786e+00 - val_acc: 0.8116\n",
      "Epoch 20/100\n",
      "45s - loss: -2.0558e+00 - acc: 0.8252 - val_loss: -1.2932e+00 - val_acc: 0.8019\n",
      "Epoch 21/100\n",
      "46s - loss: -2.0559e+00 - acc: 0.8255 - val_loss: -1.2959e+00 - val_acc: 0.8069\n",
      "Epoch 22/100\n",
      "45s - loss: -2.0617e+00 - acc: 0.8259 - val_loss: -1.2735e+00 - val_acc: 0.8058\n",
      "Epoch 23/100\n",
      "45s - loss: -2.0655e+00 - acc: 0.8260 - val_loss: -1.2449e+00 - val_acc: 0.8057\n",
      "Epoch 24/100\n",
      "45s - loss: -2.0652e+00 - acc: 0.8260 - val_loss: -1.2579e+00 - val_acc: 0.8110\n",
      "Epoch 25/100\n",
      "45s - loss: -2.0739e+00 - acc: 0.8265 - val_loss: -1.2623e+00 - val_acc: 0.8024\n",
      "Epoch 26/100\n",
      "45s - loss: -2.0687e+00 - acc: 0.8268 - val_loss: -1.2379e+00 - val_acc: 0.8109\n",
      "Epoch 27/100\n",
      "46s - loss: -2.0785e+00 - acc: 0.8270 - val_loss: -1.2342e+00 - val_acc: 0.8103\n",
      "Epoch 28/100\n",
      "45s - loss: -2.0767e+00 - acc: 0.8271 - val_loss: -1.2282e+00 - val_acc: 0.8063\n",
      "Epoch 29/100\n",
      "46s - loss: -2.0751e+00 - acc: 0.8271 - val_loss: -1.2318e+00 - val_acc: 0.8012\n",
      "Epoch 30/100\n",
      "45s - loss: -2.0800e+00 - acc: 0.8269 - val_loss: -1.2470e+00 - val_acc: 0.8100\n",
      "Epoch 31/100\n",
      "46s - loss: -2.0855e+00 - acc: 0.8274 - val_loss: -1.2554e+00 - val_acc: 0.8122\n",
      "Epoch 32/100\n",
      "46s - loss: -2.0889e+00 - acc: 0.8277 - val_loss: -1.2382e+00 - val_acc: 0.8115\n",
      "Epoch 33/100\n",
      "46s - loss: -2.0877e+00 - acc: 0.8277 - val_loss: -1.2531e+00 - val_acc: 0.8092\n",
      "Epoch 34/100\n",
      "45s - loss: -2.0931e+00 - acc: 0.8279 - val_loss: -1.2421e+00 - val_acc: 0.8088\n",
      "Epoch 35/100\n",
      "45s - loss: -2.0932e+00 - acc: 0.8278 - val_loss: -1.2364e+00 - val_acc: 0.8105\n",
      "Epoch 36/100\n",
      "46s - loss: -2.0947e+00 - acc: 0.8281 - val_loss: -1.2267e+00 - val_acc: 0.8099\n",
      "Epoch 37/100\n",
      "46s - loss: -2.0944e+00 - acc: 0.8280 - val_loss: -1.2171e+00 - val_acc: 0.8128\n",
      "Epoch 38/100\n",
      "45s - loss: -2.0942e+00 - acc: 0.8281 - val_loss: -1.2402e+00 - val_acc: 0.8125\n",
      "Epoch 39/100\n",
      "46s - loss: -2.0969e+00 - acc: 0.8283 - val_loss: -1.2383e+00 - val_acc: 0.8095\n",
      "Epoch 40/100\n",
      "45s - loss: -2.0961e+00 - acc: 0.8282 - val_loss: -1.2278e+00 - val_acc: 0.8113\n",
      "Epoch 41/100\n",
      "46s - loss: -2.0994e+00 - acc: 0.8285 - val_loss: -1.2161e+00 - val_acc: 0.8087\n",
      "Epoch 42/100\n",
      "45s - loss: -2.0961e+00 - acc: 0.8282 - val_loss: -1.2149e+00 - val_acc: 0.8139\n",
      "Epoch 43/100\n",
      "46s - loss: -2.0978e+00 - acc: 0.8284 - val_loss: -1.1801e+00 - val_acc: 0.8101\n",
      "Epoch 44/100\n",
      "45s - loss: -2.1008e+00 - acc: 0.8287 - val_loss: -1.2109e+00 - val_acc: 0.8139\n",
      "Epoch 45/100\n",
      "45s - loss: -2.0987e+00 - acc: 0.8288 - val_loss: -1.2107e+00 - val_acc: 0.8149\n",
      "Epoch 46/100\n",
      "45s - loss: -2.0993e+00 - acc: 0.8286 - val_loss: -1.2038e+00 - val_acc: 0.8084\n",
      "Epoch 47/100\n",
      "46s - loss: -2.1027e+00 - acc: 0.8289 - val_loss: -1.2057e+00 - val_acc: 0.8117\n",
      "Epoch 48/100\n",
      "46s - loss: -2.1025e+00 - acc: 0.8288 - val_loss: -1.2150e+00 - val_acc: 0.8121\n",
      "Epoch 49/100\n",
      "45s - loss: -2.1056e+00 - acc: 0.8289 - val_loss: -1.2113e+00 - val_acc: 0.8149\n",
      "Epoch 50/100\n",
      "46s - loss: -2.1041e+00 - acc: 0.8289 - val_loss: -1.2019e+00 - val_acc: 0.8091\n",
      "Epoch 51/100\n",
      "45s - loss: -2.1067e+00 - acc: 0.8290 - val_loss: -1.2113e+00 - val_acc: 0.8146\n",
      "Epoch 52/100\n",
      "46s - loss: -2.1042e+00 - acc: 0.8292 - val_loss: -1.2109e+00 - val_acc: 0.8112\n",
      "Epoch 53/100\n",
      "46s - loss: -2.1105e+00 - acc: 0.8292 - val_loss: -1.2050e+00 - val_acc: 0.8128\n",
      "Epoch 54/100\n",
      "45s - loss: -2.1095e+00 - acc: 0.8291 - val_loss: -1.2102e+00 - val_acc: 0.8092\n",
      "Epoch 55/100\n",
      "45s - loss: -2.1112e+00 - acc: 0.8291 - val_loss: -1.2245e+00 - val_acc: 0.8126\n",
      "Epoch 56/100\n",
      "46s - loss: -2.1110e+00 - acc: 0.8292 - val_loss: -1.2016e+00 - val_acc: 0.8107\n",
      "Epoch 57/100\n",
      "46s - loss: -2.1081e+00 - acc: 0.8292 - val_loss: -1.2042e+00 - val_acc: 0.8122\n",
      "Epoch 58/100\n",
      "45s - loss: -2.1100e+00 - acc: 0.8293 - val_loss: -1.1638e+00 - val_acc: 0.8163\n",
      "Epoch 59/100\n",
      "45s - loss: -2.1130e+00 - acc: 0.8294 - val_loss: -1.1977e+00 - val_acc: 0.8130\n",
      "Epoch 60/100\n",
      "46s - loss: -2.1112e+00 - acc: 0.8294 - val_loss: -1.1999e+00 - val_acc: 0.8106\n",
      "Epoch 61/100\n",
      "45s - loss: -2.0939e+00 - acc: 0.8293 - val_loss: -1.1545e+00 - val_acc: 0.8102\n",
      "Epoch 62/100\n",
      "45s - loss: -2.0872e+00 - acc: 0.8300 - val_loss: -1.2046e+00 - val_acc: 0.8153\n",
      "Epoch 63/100\n",
      "46s - loss: -2.1102e+00 - acc: 0.8294 - val_loss: -1.2182e+00 - val_acc: 0.8132\n",
      "Epoch 64/100\n",
      "46s - loss: -2.1135e+00 - acc: 0.8294 - val_loss: -1.2302e+00 - val_acc: 0.8138\n",
      "Epoch 65/100\n",
      "45s - loss: -2.1153e+00 - acc: 0.8295 - val_loss: -1.2123e+00 - val_acc: 0.8123\n",
      "Epoch 66/100\n",
      "45s - loss: -2.1155e+00 - acc: 0.8295 - val_loss: -1.2107e+00 - val_acc: 0.8127\n",
      "Epoch 67/100\n",
      "45s - loss: -2.1141e+00 - acc: 0.8294 - val_loss: -1.2012e+00 - val_acc: 0.8132\n",
      "Epoch 68/100\n",
      "45s - loss: -2.1120e+00 - acc: 0.8295 - val_loss: -1.1725e+00 - val_acc: 0.8148\n",
      "Epoch 69/100\n",
      "45s - loss: -2.1170e+00 - acc: 0.8297 - val_loss: -1.1737e+00 - val_acc: 0.8149\n",
      "Epoch 70/100\n",
      "46s - loss: -2.1149e+00 - acc: 0.8296 - val_loss: -1.1954e+00 - val_acc: 0.8161\n",
      "Epoch 71/100\n",
      "45s - loss: -2.1185e+00 - acc: 0.8297 - val_loss: -1.1891e+00 - val_acc: 0.8146\n",
      "Epoch 72/100\n",
      "46s - loss: -2.1180e+00 - acc: 0.8296 - val_loss: -1.2025e+00 - val_acc: 0.8172\n",
      "Epoch 73/100\n",
      "46s - loss: -2.1173e+00 - acc: 0.8298 - val_loss: -1.1658e+00 - val_acc: 0.8120\n",
      "Epoch 74/100\n",
      "46s - loss: -2.1165e+00 - acc: 0.8297 - val_loss: -1.1813e+00 - val_acc: 0.8131\n",
      "Epoch 75/100\n",
      "46s - loss: -2.1171e+00 - acc: 0.8299 - val_loss: -1.1613e+00 - val_acc: 0.8082\n",
      "Epoch 76/100\n",
      "46s - loss: -2.1161e+00 - acc: 0.8297 - val_loss: -1.1916e+00 - val_acc: 0.8124\n",
      "Epoch 77/100\n",
      "46s - loss: -2.1123e+00 - acc: 0.8294 - val_loss: -1.1935e+00 - val_acc: 0.8112\n",
      "Epoch 78/100\n",
      "45s - loss: -2.1128e+00 - acc: 0.8293 - val_loss: -1.2018e+00 - val_acc: 0.8128\n",
      "Epoch 79/100\n",
      "46s - loss: -2.1125e+00 - acc: 0.8293 - val_loss: -1.1737e+00 - val_acc: 0.8108\n",
      "Epoch 80/100\n",
      "45s - loss: -2.1168e+00 - acc: 0.8296 - val_loss: -1.2043e+00 - val_acc: 0.8116\n",
      "Epoch 81/100\n",
      "45s - loss: -2.1192e+00 - acc: 0.8299 - val_loss: -1.2066e+00 - val_acc: 0.8141\n",
      "Epoch 82/100\n",
      "45s - loss: -2.1234e+00 - acc: 0.8300 - val_loss: -1.2100e+00 - val_acc: 0.8125\n",
      "Epoch 83/100\n",
      "46s - loss: -2.1218e+00 - acc: 0.8299 - val_loss: -1.2134e+00 - val_acc: 0.8145\n",
      "Epoch 84/100\n",
      "45s - loss: -2.1224e+00 - acc: 0.8299 - val_loss: -1.2072e+00 - val_acc: 0.8129\n",
      "Epoch 85/100\n",
      "46s - loss: -2.1228e+00 - acc: 0.8300 - val_loss: -1.1996e+00 - val_acc: 0.8180\n",
      "Epoch 86/100\n",
      "46s - loss: -2.1229e+00 - acc: 0.8300 - val_loss: -1.1991e+00 - val_acc: 0.8169\n",
      "Epoch 87/100\n",
      "46s - loss: -2.1200e+00 - acc: 0.8299 - val_loss: -1.2020e+00 - val_acc: 0.8139\n",
      "Epoch 88/100\n",
      "46s - loss: -2.1224e+00 - acc: 0.8299 - val_loss: -1.2129e+00 - val_acc: 0.8166\n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45s - loss: -2.1207e+00 - acc: 0.8299 - val_loss: -1.2074e+00 - val_acc: 0.8112\n",
      "Epoch 90/100\n",
      "45s - loss: -2.1227e+00 - acc: 0.8300 - val_loss: -1.2105e+00 - val_acc: 0.8141\n",
      "Epoch 91/100\n",
      "46s - loss: -2.1192e+00 - acc: 0.8297 - val_loss: -1.2132e+00 - val_acc: 0.8144\n",
      "Epoch 92/100\n",
      "45s - loss: -2.1224e+00 - acc: 0.8301 - val_loss: -1.2019e+00 - val_acc: 0.8111\n",
      "Epoch 93/100\n",
      "46s - loss: -2.1167e+00 - acc: 0.8296 - val_loss: -1.1692e+00 - val_acc: 0.8161\n",
      "Epoch 94/100\n",
      "45s - loss: -2.1200e+00 - acc: 0.8299 - val_loss: -1.1805e+00 - val_acc: 0.8148\n",
      "Epoch 95/100\n",
      "46s - loss: -2.1104e+00 - acc: 0.8294 - val_loss: -1.1698e+00 - val_acc: 0.8132\n",
      "Epoch 96/100\n",
      "45s - loss: -2.1133e+00 - acc: 0.8293 - val_loss: -1.1722e+00 - val_acc: 0.8116\n",
      "Epoch 97/100\n",
      "45s - loss: -2.1135e+00 - acc: 0.8295 - val_loss: -1.1839e+00 - val_acc: 0.8125\n",
      "Epoch 98/100\n",
      "45s - loss: -2.1226e+00 - acc: 0.8301 - val_loss: -1.2070e+00 - val_acc: 0.8126\n",
      "Epoch 99/100\n",
      "46s - loss: -2.1244e+00 - acc: 0.8303 - val_loss: -1.1940e+00 - val_acc: 0.8135\n",
      "Epoch 100/100\n",
      "45s - loss: -2.1236e+00 - acc: 0.8302 - val_loss: -1.1938e+00 - val_acc: 0.8134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3bcd392110>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=128, verbose=2,callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "model.save('cnn_model_fr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling/ Importing the model\n",
    "model = load_model('cnn_model_fr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.34%\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import traceback\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "  \"\"\"\n",
    "  String cleaning before vectorization\n",
    "  \"\"\"\n",
    "  try:\n",
    "    string = re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    string = re.sub(r\"[^a-zA-ZÃ-Ã¿0-9]\", \" \", string) \n",
    "    #string = re.sub(r\"\\W+\", \" \", string)   \n",
    "    #return words list\n",
    "    return string\n",
    "  except:\n",
    "    print (traceback.print_exc())\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training and Test data\n",
    "import pandas as pd\n",
    "data=pd.read_csv('tweets_sentiment.txt', header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data['text']\n",
    "a=df[df.str.contains('macron' or 'emmanuelmacron' or 'enmarche' or '@emmanuelmacron' or '#emmanuelmacron' or 'mlpofficiel' or 'mlp_officiel' or 'lepen' or 'marine2017' or '@mlp_officiel' or 'mlp',na=False)]\n",
    "data['text2']=a\n",
    "data=data.dropna()\n",
    "data['text']=data['text2']\n",
    "del data['text2']\n",
    "data['text'] = data['text'].apply(clean_str)\n",
    "data.sentiment=data.sentiment.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ah macron et ses approximations</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a marseille fillon cible macron lepen et mÃ©le...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>face mÃ©lenchon et son programme de destruction...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>sÃ©rieusement venant des soutiens de macron l Ã©...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  sentiment\n",
       "2                    ah macron et ses approximations           1\n",
       "9    a marseille fillon cible macron lepen et mÃ©le...          1\n",
       "16  face mÃ©lenchon et son programme de destruction...          1\n",
       "18   macron m a fait rÃ©flÃ©chir sur quand mÃ©lenchon...          1\n",
       "39  sÃ©rieusement venant des soutiens de macron l Ã©...          1"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)\n",
    "data=data[500:574054]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>l agitation mÃ©diatique autour de macron et de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>simarinenestpasausecondtour on aura ce genre ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>sondage macron et le pen en baisse mais toujou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>simarinenestpasausecondtour on aura ce genre ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>racisme islam cdanslair islamophobie avecmari...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment\n",
       "500   l agitation mÃ©diatique autour de macron et de...          1\n",
       "501   simarinenestpasausecondtour on aura ce genre ...          1\n",
       "502  sondage macron et le pen en baisse mais toujou...          1\n",
       "503   simarinenestpasausecondtour on aura ce genre ...          1\n",
       "504   racisme islam cdanslair islamophobie avecmari...          1"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>l agitation mÃ©diatique autour de macron et de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>simarinenestpasausecondtour on aura ce genre ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>sondage macron et le pen en baisse mais toujou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>simarinenestpasausecondtour on aura ce genre ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>racisme islam cdanslair islamophobie avecmari...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  sentiment\n",
       "500   l agitation mÃ©diatique autour de macron et de...          1\n",
       "501   simarinenestpasausecondtour on aura ce genre ...          1\n",
       "502  sondage macron et le pen en baisse mais toujou...          1\n",
       "503   simarinenestpasausecondtour on aura ce genre ...          1\n",
       "504   racisme islam cdanslair islamophobie avecmari...          1"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')\n",
    "    \n",
    "max_fatures = 2000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "X = tokenizer.texts_to_sequences(data['text'].values)\n",
    "X = pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:6: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(196, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 39, 140)           280000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 196)               264208    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 544,602\n",
      "Trainable params: 544,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 140\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='./Graph/LSTM', histogram_freq=0, write_graph=True, write_images=True)\n",
    "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1], dropout=0.2))\n",
    "#model.add(keras.layers.SpatialDropout1D(0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.2, dropout_W=0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((384281, 39), (384281, 2))\n",
      "((189273, 39), (189273, 2))\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['sentiment']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17\n",
      "848s - loss: 0.0360 - acc: 0.9871\n",
      "Epoch 2/17\n",
      "855s - loss: 0.0363 - acc: 0.9872\n",
      "Epoch 3/17\n",
      "855s - loss: 0.0360 - acc: 0.9873\n",
      "Epoch 4/17\n",
      "851s - loss: 0.0362 - acc: 0.9871\n",
      "Epoch 5/17\n",
      "851s - loss: 0.0367 - acc: 0.9868\n",
      "Epoch 6/17\n",
      "850s - loss: 0.0372 - acc: 0.9866\n",
      "Epoch 7/17\n",
      "849s - loss: 0.0382 - acc: 0.9864\n",
      "Epoch 8/17\n",
      "847s - loss: 0.0379 - acc: 0.9863\n",
      "Epoch 9/17\n",
      "843s - loss: 0.0384 - acc: 0.9863\n",
      "Epoch 10/17\n",
      "847s - loss: 0.0384 - acc: 0.9862\n",
      "Epoch 11/17\n",
      "843s - loss: 0.0395 - acc: 0.9858\n",
      "Epoch 12/17\n",
      "845s - loss: 0.0397 - acc: 0.9859\n",
      "Epoch 13/17\n",
      "847s - loss: 0.0399 - acc: 0.9858\n",
      "Epoch 14/17\n",
      "846s - loss: 0.0402 - acc: 0.9856\n",
      "Epoch 15/17\n",
      "847s - loss: 0.0405 - acc: 0.9854\n",
      "Epoch 16/17\n",
      "847s - loss: 0.0400 - acc: 0.9858\n",
      "Epoch 17/17\n",
      "846s - loss: 0.0410 - acc: 0.9854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3bd32cd990>"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs = 17, batch_size=batch_size, verbose = 2,callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "from keras.models import load_model\n",
    "import h5py\n",
    "model.save('lstm_model_fr.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executing LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling/ Importing the model\n",
    "model = load_model('lstm_model_fr.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.08\n",
      "acc: 0.98\n"
     ]
    }
   ],
   "source": [
    "validation_size = 1600\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos_acc', 99.63017751479289, '%')\n",
      "('neg_acc', 93.95161290322581, '%')\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "from __future__ import division\n",
    "for x in range(len(X_validate)):\n",
    "    \n",
    "    result = model.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "   \n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0:\n",
    "            neg_correct += 1\n",
    "        else:\n",
    "            pos_correct += 1\n",
    "       \n",
    "    if np.argmax(Y_validate[x]) == 0:\n",
    "        neg_cnt += 1\n",
    "    else:\n",
    "        pos_cnt += 1\n",
    "\n",
    "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
